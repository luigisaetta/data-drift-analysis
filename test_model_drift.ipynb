{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4fb647",
   "metadata": {},
   "source": [
    "### Model Drift Analysis\n",
    "\n",
    "Model Drift Analysis require two dataset containing not only the features (xi) but also the target.\n",
    "\n",
    "It means that, in order to monitor Model's performances and detect Model drift we need, in some way, to collect data and analyze the results in order to define the \"ground truth\".\n",
    "\n",
    "In this NB I have put a prototype that can be used to start working on Model Drift.\n",
    "\n",
    "The dataset used is again the Employee Attrition Data and the model is based on LightGBM (GBM).\n",
    "\n",
    "We simulate a Data Drift (adding a \"shift\" to some features, in order to make performances worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf99d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import ads\n",
    "\n",
    "# used to serialize the pipeline\n",
    "from pickle import dump, load\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import get_scorer, make_scorer, f1_score, roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# added to handle with pipelines\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fdba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# definisco le funzioni che identificano le categorie di colonne\n",
    "#\n",
    "def cat_cols_selector(df, target_name):\n",
    "    # the input is the dataframe\n",
    "    \n",
    "    # cols with less than THR values are considered categoricals\n",
    "    THR = 10\n",
    "    \n",
    "    nunique = df.nunique()\n",
    "    types = df.dtypes\n",
    "    \n",
    "    col_list = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if ((types[col] == 'object') or (nunique[col] < THR)):\n",
    "            # print(col)\n",
    "            if col != target_name:\n",
    "                col_list.append(col)\n",
    "    \n",
    "    return col_list\n",
    "\n",
    "def num_cols_selector(df, target_name):\n",
    "    THR = 10\n",
    "    \n",
    "    types = df.dtypes\n",
    "    nunique = df.nunique()\n",
    "    \n",
    "    col_list = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if (types[col] != 'object') and (nunique[col] >= THR): \n",
    "            # print(col)\n",
    "            if col != target_name:\n",
    "                col_list.append(col)\n",
    "    \n",
    "    return col_list\n",
    "\n",
    "def load_as_dataframe(path):\n",
    "    ds = DatasetFactory.open(path,\n",
    "                             target=\"Attrition\").set_positive_class('Yes')\n",
    "\n",
    "    ds_up = ds.up_sample()\n",
    "\n",
    "    # drop unneeded columns\n",
    "    cols_to_drop = ['Directs','name', 'Over18','WeeklyWorkedHours','EmployeeNumber']\n",
    "\n",
    "    ds_used = ds_up.drop(columns=cols_to_drop)\n",
    "    \n",
    "    df_used = ds_used.to_pandas_dataframe()\n",
    "    \n",
    "    \n",
    "\n",
    "    # train, test split (lo faccio direttamente sui dataframe)\n",
    "    df_train, df_test = train_test_split(df_used, shuffle=True, test_size=0.2, random_state = 1234)\n",
    "\n",
    "    print(\"# of samples in train set\", df_train.shape[0])\n",
    "    print(\"# of samples in test set\", df_test.shape[0])\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d4f992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82640dca164480c8ebd9448d8909eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples in train set 1972\n",
      "# of samples in test set 494\n",
      "\n",
      "Numerical columns: ['Age', 'SalaryLevel', 'CommuteLength', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'YearsinIndustry', 'YearsOnJob', 'YearsAtCurrentLevel', 'YearsSinceLastPromotion', 'YearsWithCurrManager'] (13)\n",
      "\n",
      "Categorical columns: ['TravelForWork', 'JobFunction', 'EducationalLevel', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance'] (17)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and do upsampling\n",
    "TARGET = 'Attrition'\n",
    "\n",
    "attrition_path = \"/opt/notebooks/ads-examples/oracle_data/orcl_attrition.csv\"\n",
    "\n",
    "df_train, df_test = load_as_dataframe(attrition_path)\n",
    "\n",
    "# uso ancora la classe dataset per fare l'upsampling\n",
    "\n",
    "cat_cols = cat_cols_selector(df_train, TARGET)\n",
    "num_cols = num_cols_selector(df_train, TARGET)\n",
    "\n",
    "print()\n",
    "print(f'Numerical columns: {num_cols} ({len(num_cols)})')\n",
    "print()\n",
    "print(f'Categorical columns: {cat_cols} ({len(cat_cols)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8267e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# creo la parte Transformers per le pipeline\n",
    "#\n",
    "\n",
    "# per questo dataset non vi sono missing values\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('standard_scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a37683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop([TARGET], axis=1), df_train[TARGET]\n",
    "X_test, y_test = df_test.drop([TARGET], axis=1), df_test[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc0e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# definisco la pipeline completa\n",
    "#\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                           ('clf', lgb.LGBMClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578cba40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standard_scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Age', 'SalaryLevel',\n",
       "                                                   'CommuteLength',\n",
       "                                                   'HourlyRate',\n",
       "                                                   'MonthlyIncome',\n",
       "                                                   'MonthlyRate',\n",
       "                                                   'NumCompaniesWorked',\n",
       "                                                   'PercentSalaryHike',\n",
       "                                                   'YearsinIndustry',\n",
       "                                                   'YearsOnJob',\n",
       "                                                   'YearsAtCurrentLevel',\n",
       "                                                   'YearsSinceLastPromoti...\n",
       "                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n",
       "                                                                                  unknown_value=-1))]),\n",
       "                                                  ['TravelForWork',\n",
       "                                                   'JobFunction',\n",
       "                                                   'EducationalLevel',\n",
       "                                                   'EducationField',\n",
       "                                                   'EnvironmentSatisfaction',\n",
       "                                                   'Gender', 'JobInvolvement',\n",
       "                                                   'JobLevel', 'JobRole',\n",
       "                                                   'JobSatisfaction',\n",
       "                                                   'MaritalStatus', 'OverTime',\n",
       "                                                   'PerformanceRating',\n",
       "                                                   'RelationshipSatisfaction',\n",
       "                                                   'StockOptionLevel',\n",
       "                                                   'TrainingTimesLastYear',\n",
       "                                                   'WorkLifeBalance'])])),\n",
       "                ('clf', LGBMClassifier())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c578bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set result:\n",
      "Acc: 0.9494, AUC: 0.9951\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf.predict(X_test)\n",
    "test_probas = clf.predict_proba(X_test)\n",
    "\n",
    "print('Validation set result:')\n",
    "\n",
    "roc_auc = round(roc_auc_score(y_test, test_probas[:,1]), 4)\n",
    "acc = round(accuracy_score(y_test, test_pred), 4)\n",
    "\n",
    "print(f\"Acc: {acc}, AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1fece68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in a file the metrics computed on the reference set\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "dict_ref = [{\n",
    "    \"ts_date\": now,\n",
    "    \"model_name\": \"lgb1\",\n",
    "    \"algorithm\": \"lightgbm\",\n",
    "    \"accuracy\": acc,\n",
    "    \"roc_auc\": roc_auc\n",
    "}]\n",
    "\n",
    "df_ref = pd.DataFrame(dict_ref)\n",
    "\n",
    "# save initial file\n",
    "df_ref.to_csv(\"model_metrics.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439dc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pipeline\n",
    "dump(clf, open(\"pipe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd0027",
   "metadata": {},
   "source": [
    "### Second dataset analysis\n",
    "\n",
    "we should load the model from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0db0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standard_scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Age', 'SalaryLevel',\n",
       "                                                   'CommuteLength',\n",
       "                                                   'HourlyRate',\n",
       "                                                   'MonthlyIncome',\n",
       "                                                   'MonthlyRate',\n",
       "                                                   'NumCompaniesWorked',\n",
       "                                                   'PercentSalaryHike',\n",
       "                                                   'YearsinIndustry',\n",
       "                                                   'YearsOnJob',\n",
       "                                                   'YearsAtCurrentLevel',\n",
       "                                                   'YearsSinceLastPromoti...\n",
       "                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n",
       "                                                                                  unknown_value=-1))]),\n",
       "                                                  ['TravelForWork',\n",
       "                                                   'JobFunction',\n",
       "                                                   'EducationalLevel',\n",
       "                                                   'EducationField',\n",
       "                                                   'EnvironmentSatisfaction',\n",
       "                                                   'Gender', 'JobInvolvement',\n",
       "                                                   'JobLevel', 'JobRole',\n",
       "                                                   'JobSatisfaction',\n",
       "                                                   'MaritalStatus', 'OverTime',\n",
       "                                                   'PerformanceRating',\n",
       "                                                   'RelationshipSatisfaction',\n",
       "                                                   'StockOptionLevel',\n",
       "                                                   'TrainingTimesLastYear',\n",
       "                                                   'WorkLifeBalance'])])),\n",
       "                ('clf', LGBMClassifier())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the pipeline\n",
    "clf = load(open(\"pipe.pkl\", \"rb\"))\n",
    "\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1f52e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999836f2e9464b4bac1e7127415a82e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples in train set 1972\n",
      "# of samples in test set 494\n"
     ]
    }
   ],
   "source": [
    "# restart from the dataset (in reality we'll have a new dataset, here we're simulating the changes)\n",
    "df_train, df_test = load_as_dataframe(attrition_path)\n",
    "\n",
    "X_test, y_test = df_test.drop([TARGET], axis=1), df_test[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e1a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate some changes in the dataset\n",
    "# we use again the test set, but with a \"Data Drift\"\n",
    "\n",
    "X_test['SalaryLevel'] = X_test['SalaryLevel'] - 2000\n",
    "X_test['Age'] = X_test['Age'] + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874766ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set result:\n",
      "Acc: 0.9372, AUC: 0.9788\n"
     ]
    }
   ],
   "source": [
    "# scoring: compute new metrics\n",
    "test_pred = clf.predict(X_test)\n",
    "test_probas = clf.predict_proba(X_test)\n",
    "\n",
    "print('Validation set result:')\n",
    "\n",
    "roc_auc = round(roc_auc_score(y_test, test_probas[:,1]), 4)\n",
    "acc = round(accuracy_score(y_test, test_pred), 4)\n",
    "\n",
    "print(f\"Acc: {acc}, AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7f78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a second row to the file\n",
    "\n",
    "# read old file\n",
    "model_metrics = pd.read_csv(\"model_metrics.csv\")\n",
    "\n",
    "# compute new row\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "dict_ref = [{\n",
    "    \"ts_date\": now,\n",
    "    \"model_name\": \"lgb1\",\n",
    "    \"algorithm\": \"lightgbm\",\n",
    "    \"accuracy\": acc,\n",
    "    \"roc_auc\": roc_auc\n",
    "}]\n",
    "\n",
    "# new df\n",
    "df_new_metrics = pd.DataFrame(dict_ref)\n",
    "\n",
    "new_model_metrics = pd.concat([model_metrics, df_new_metrics])\n",
    "\n",
    "# save to an updated file\n",
    "new_model_metrics.to_csv(\"model_metrics.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d4ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
